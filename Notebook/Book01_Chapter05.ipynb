{"cells":[{"source":"# Change directory to VSCode workspace root so that relative path loads work correctly. Turn this addition off with the DataScience.changeDirOnImportExport setting\r\nimport os\r\ntry:\r\n\tos.chdir(os.path.join(os.getcwd(), '..\\..'))\r\n\tprint(os.getcwd())\r\nexcept:\r\n\tpass\r\n","cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ##Logistic Regression"],"metadata":{}},{"source":["import numpy as np \n","import matplotlib.pyplot as plt\n","import scipy as sp\n","import h5py\n","import os\n","from PIL import Image\n","from scipy import ndimage\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Load training and test datasets"],"metadata":{}},{"source":["def load_dataset():\n","    script_dir = os.getcwd()\n","    train_file_path = os.path.join(script_dir, 'MLNotes', 'datasets', 'train_catvnoncat.h5')\n","    test_file_path = os.path.join(script_dir, 'MLNotes', 'datasets', 'test_catvnoncat.h5')\n","\n","    train_dataset = h5py.File(train_file_path, \"r\")\n","    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n","    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n","\n","    test_dataset = h5py.File(test_file_path, \"r\")\n","    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n","    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n","\n","    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n","    \n","    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n","    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n","    \n","    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Loading the data (cat/non-cat)\n","train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# display sample training image\n","def sample_training_data(index):\n","    c = classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\")\n","    plt.imshow(train_set_x_orig[index])\n","    plt.text(2,10, c, fontsize=24)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["sample_training_data(2)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["sample_training_data(3)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# dataset info\n","def display_dataset_info():\n","    m_train = train_set_x_orig.shape[0]\n","    m_test = test_set_x_orig.shape[0]\n","    num_px = train_set_x_orig.shape[1]\n","    print (\"Number of training examples: m_train = \" + str(m_train))\n","    print (\"Number of testing examples: m_test = \" + str(m_test))\n","    print (\"Height/Width of each image: num_px = \" + str(num_px))\n","    print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n","    print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n","    print (\"train_set_y shape: \" + str(train_set_y.shape))\n","    print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n","    print (\"test_set_y shape: \" + str(test_set_y.shape))\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["display_dataset_info()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Reshape the training and test data sets so that\n"," images of size (num_px, num_px, 3) are flattened into\n"," single vectors of shape (num_px  ∗∗  num_px  ∗∗ 3, 1).\n"," How to  flatten a matrix X of shape (a,b,c,d)\n"," to a matrix X_flatten of shape (b ∗∗ c ∗∗ d, a)\n"," X_flatten = X.reshape(X.shape[0], -1).T\n"," where X.T is the transpose of X"],"metadata":{}},{"source":["def flatten_dataset():\n","    train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n","    test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n","    return train_set_x_flatten, test_set_x_flatten\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["train_set_x_flatten, test_set_x_flatten = flatten_dataset()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n","print (\"train_set_y shape: \" + str(train_set_y.shape))\n","print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n","print (\"test_set_y shape: \" + str(test_set_y.shape))\n","print (\"sanity check after reshaping: \" + str(train_set_x_flatten[0:5,0]))\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" To represent color images, the red, green and blue channels\n"," (RGB) must be specified for each pixel, and so the pixel value\n"," is actually a vector of three numbers ranging from 0 to 255.\n"," One common preprocessing step in machine learning is to center and standardize\n"," your dataset, meaning that you substract the mean of the whole numpy array\n"," from each example, and then divide each example\n"," by the standard deviation of the whole numpy array.\n"," But for picture datasets, it is simpler and more convenient\n"," and works almost as well to just divide\n"," every row of the dataset by 255 (the maximum value of a pixel channel)"],"metadata":{}},{"cell_type":"markdown","source":[" Standardized dataset"],"metadata":{}},{"source":["train_set_x = train_set_x_flatten/255.\n","test_set_x = test_set_x_flatten/255.\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Main steps to build a neural network\n"," 1. Define the model structure (such as number of input features)\n"," 2. Initialize the model's parameters\n"," 3. Loop:\n"," * Calculate current loss (forward propagation)\n"," * Calculate current gradient (backward propagation)\n"," * Update parameters (gradient descent)"],"metadata":{}},{"cell_type":"markdown","source":[" Sigmoid function\n"," σ(x)= 1 / (1 + exp(-x))"],"metadata":{}},{"source":["# use np.exp instead of math.exp\n","# so that the function works with vectors as well as scalars\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["xs = np.linspace(-5,5)\n","plt.plot(xs, sigmoid(xs))\n","plt.show()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Sigmoid derivative function\n"," Derivative of the sigmoid function\n"," with respect to its input x\n"," σ′(x) = σ(x)(1−σ(x))"],"metadata":{}},{"source":["def sigmoid_derivative(x):\n","    s = sigmoid(x)\n","    return s * (1-s)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["xs = np.linspace(-5,5)\n","plt.plot(xs, sigmoid_derivative(xs))\n","plt.show()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Unroll 3D array (image) into a 1D vector"],"metadata":{}},{"source":["def image2vector(image):\n","    \"\"\"\n","    Argument:\n","    image -- a numpy array of shape (length, height, depth)\n","    \n","    Returns:\n","    v -- a vector of shape (length*height*depth, 1)\n","    \"\"\"\n","\n","    v = image.reshape(image.shape[0] * image.shape[1] * image.shape[2], 1)\n","    return v\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Normalize vectors"],"metadata":{}},{"source":["def normalizeRows(x):\n","    \"\"\"\n","    A function that normalizes each row of the matrix x\n","    \n","    Argument:\n","    x -- A numpy matrix of shape (n, m)\n","    \n","    Returns:\n","    x -- The normalized (by row) numpy matrix.\n","    \"\"\"\n","    \n","    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n","    x_norm = np.linalg.norm(x, axis=1, keepdims=True)\n","    # Divide x by its norm.\n","    x = x/x_norm\n","    return x\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Softmax function"],"metadata":{}},{"source":["def softmax(x):\n","    \"\"\"Calculates the softmax for each row of the input x.\n","\n","    Argument:\n","    x -- A numpy matrix of shape (n,m)\n","\n","    Returns:\n","    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n","    \"\"\"\n","    \n","    # Apply exp() element-wise to x.\n","    x_exp = np.exp(x)\n","\n","    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n","    x_sum = np.sum(x_exp, axis = 1, keepdims = True)\n","    \n","    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n","    s = x_exp / x_sum   \n","    return s\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" L1 and L2 loss functions\n"," L1(ŷ ,y) = ∑i=0 m |y(i)−ŷ (i)|\n"," L2(ŷ ,y) = ∑i=0 m (y(i)−ŷ (i))**2"],"metadata":{}},{"source":["def L1(yhat, y):\n","    \"\"\"\n","    Arguments:\n","    yhat -- vector of size m (predicted labels)\n","    y -- vector of size m (true labels)\n","    \n","    Returns:\n","    loss -- the value of the L1 loss function defined above\n","    \"\"\"\n","    \n","    loss = np.sum(abs(y-yhat))\n","    return loss\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["def L2(yhat, y):\n","    \"\"\"\n","    Arguments:\n","    yhat -- vector of size m (predicted labels)\n","    y -- vector of size m (true labels)\n","    \n","    Returns:\n","    loss -- the value of the L2 loss function defined above\n","    \"\"\"\n","    \n","    loss = np.sum(np.power((y-yhat),2))\n","    return loss\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Initialize parameters\n"," For image inputs, w will be of shape (num_px  ××  num_px  ××  3, 1)"],"metadata":{}},{"source":["def initialize_with_zeros(dim):\n","    \"\"\"\n","    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n","    \n","    Argument:\n","    dim -- size of the w vector we want (or number of parameters in this case)\n","    \n","    Returns:\n","    w -- initialized vector of shape (dim, 1)\n","    b -- initialized scalar (corresponds to the bias)\n","    \"\"\"\n","    \n","    w = np.zeros((dim, 1))\n","    b = 0\n","\n","    assert(w.shape == (dim, 1))\n","    assert(isinstance(b, float) or isinstance(b, int))\n","    \n","    return w, b\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Forward and Backward propagation\n"," Forward Propagation\n"," * Get X\n"," * Compute compute  A=σ(wTX+b)=(a(1),a(2),...,a(m−1),a(m))A=σ(wTX+b)=(a(1),a(2),...,a(m−1),a(m))\n"," * Calculate the cost function:  J=−1m∑mi=1y(i)log(a(i))+(1−y(i))log(1−a(i))J=−1m∑i=1my(i)log⁡(a(i))+(1−y(i))log⁡(1−a(i))"],"metadata":{}},{"source":["def propagate(w, b, X, Y):\n","    \"\"\"\n","    Implement the cost function and its gradient for the propagation explained above\n","\n","    Arguments:\n","    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n","    b -- bias, a scalar\n","    X -- data of size (num_px * num_px * 3, number of examples)\n","    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n","\n","    Return:\n","    cost -- negative log-likelihood cost for logistic regression\n","    dw -- gradient of the loss with respect to w, thus same shape as w\n","    db -- gradient of the loss with respect to b, thus same shape as b\n","    \n","    \"\"\"\n","    \n","    m = X.shape[1]\n","    \n","    # FORWARD PROPAGATION (FROM X TO COST)\n","    # compute activation\n","    A = sigmoid(np.dot(w.T,X) + b)\n","    # compute cost\n","    cost = (-1/m) * np.sum( (Y *np.log(A)) + ((1-Y) * np.log(1-A)) )\n","    \n","    # BACKWARD PROPAGATION (TO FIND GRAD)\n","    # derivatives of w and b\n","    db = (1/m) * (np.sum(A-Y))\n","    dw = (1/m) * (np.dot(X, np.subtract(A,Y).T))\n","    \n","    assert(db.dtype == float)\n","    assert(dw.shape == w.shape)\n","    \n","    cost = np.squeeze(cost)\n","    assert(cost.shape == ())\n","\n","    grads = {\"dw\": dw,\n","             \"db\": db}\n","    \n","    return grads, cost\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Optimization\n"," The goal is to learn $w$ and $b$ by minimizing the cost function $J$.\n"," For a parameter $\\theta$, the update rule is\n"," $ \\theta = \\theta - \\alpha \\text{ } d\\theta$,\n"," where $\\alpha$ is the learning rate."],"metadata":{}},{"source":["def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n","    \"\"\"\n","    This function optimizes w and b by running a gradient descent algorithm\n","    \n","    Arguments:\n","    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n","    b -- bias, a scalar\n","    X -- data of shape (num_px * num_px * 3, number of examples)\n","    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n","    num_iterations -- number of iterations of the optimization loop\n","    learning_rate -- learning rate of the gradient descent update rule\n","    print_cost -- True to print the loss every 100 steps\n","    \n","    Returns:\n","    params -- dictionary containing the weights w and bias b\n","    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n","    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n","    \n","    Tips:\n","    You basically need to write down two steps and iterate through them:\n","        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n","        2) Update the parameters using gradient descent rule for w and b.\n","    \"\"\"\n","    \n","    costs = []\n","    \n","    for i in range(num_iterations):\n","        # Cost and gradient calculation (≈ 1-4 lines of code)\n","        grads, cost = propagate(w, b, X, Y)\n","        \n","        # Retrieve derivatives from grads\n","        dw = grads[\"dw\"]\n","        db = grads[\"db\"]\n","        \n","        # update rule\n","        w = w - learning_rate * dw\n","        b = b - learning_rate * db\n","        \n","        # Record the costs\n","        if i % 100 == 0:\n","            costs.append(cost)\n","        \n","        # Print the cost every 100 training iterations\n","        if print_cost and i % 100 == 0:\n","            print (\"Cost after iteration %i: %f\" %(i, cost))\n","    \n","    params = {\"w\": w,\n","              \"b\": b}\n","    \n","    grads = {\"dw\": dw,\n","             \"db\": db}\n","    \n","    return params, grads, costs\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Prediction\n"," 1. Calculate  Ŷ =A=σ(wTX+b)Y^=A=σ(wTX+b)\n"," 2. Convert the entries of a into 0 (if activation <= 0.5)\n"," or 1 (if activation > 0.5),\n"," stores the predictions in a vector Y_prediction"],"metadata":{}},{"source":["def predict(w, b, X):\n","    '''\n","    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n","    \n","    Arguments:\n","    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n","    b -- bias, a scalar\n","    X -- data of size (num_px * num_px * 3, number of examples)\n","    \n","    Returns:\n","    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n","    '''\n","    \n","    m = X.shape[1]\n","    Y_prediction = np.zeros((1,m))\n","    w = w.reshape(X.shape[0], 1)\n","    \n","    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n","    A = sigmoid(np.dot(w.T, X) + b)\n","    \n","    for i in range(A.shape[1]):\n","        \n","        # Convert probabilities A[0,i] to actual predictions p[0,i]\n","        Y_prediction[0, i] = 1 if A[0, i] > 0.5 else 0\n","    \n","    assert(Y_prediction.shape == (1, m))\n","    \n","    return Y_prediction\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Model creation\n"," Merge all above functions into a model"],"metadata":{}},{"source":["def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n","    \"\"\"\n","    Builds the logistic regression model by calling the function you've implemented previously\n","    \n","    Arguments:\n","    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n","    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n","    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n","    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n","    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n","    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n","    print_cost -- Set to true to print the cost every 100 iterations\n","    \n","    Returns:\n","    d -- dictionary containing information about the model.\n","    \"\"\"\n","    \n","    # initialize parameters with zeros (≈ 1 line of code)\n","    w, b = initialize_with_zeros(X_train.shape[0])\n","\n","    # Gradient descent (≈ 1 line of code)\n","    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n","    \n","    # Retrieve parameters w and b from dictionary \"parameters\"\n","    w = parameters[\"w\"]\n","    b = parameters[\"b\"]\n","    \n","    # Predict test/train set examples (≈ 2 lines of code)\n","    Y_prediction_test = predict(w, b, X_test)\n","    Y_prediction_train = predict(w, b, X_train)\n","\n","    # Print train/test Errors\n","    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n","    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n","\n","    \n","    d = {\"costs\": costs,\n","         \"Y_prediction_test\": Y_prediction_test, \n","         \"Y_prediction_train\" : Y_prediction_train, \n","         \"w\" : w, \n","         \"b\" : b,\n","         \"learning_rate\" : learning_rate,\n","         \"num_iterations\": num_iterations}\n","    \n","    return d\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" #Testing"],"metadata":{}},{"source":["d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["d[\"Y_prediction_test\"]\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# display sample test image and prediction result\n","def sample_test_data(index):\n","    num_px = train_set_x_orig.shape[1]\n","    c = classes[d[\"Y_prediction_test\"][0,index]].decode(\"utf-8\")\n","    plt.imshow(test_set_x[:,index].reshape((num_px, num_px, 3)))\n","    plt.text(2,10, c, fontsize=24)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["sample_test_data(5)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Plot learning curve (with costs)\n","costs = np.squeeze(d['costs'])\n","plt.plot(costs)\n","plt.ylabel('cost')\n","plt.xlabel('iterations (per hundreds)')\n","plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n","plt.show()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Choice of learning rate\n"," The learning rate  αα  determines how rapidly we update\n"," the parameters. If the learning rate is too large\n"," we may \"overshoot\" the optimal value. Similarly, if it is\n"," too small we will need too many iterations to converge\n"," to the best values.\n"," That's why it is crucial to use a well-tuned learning rate."],"metadata":{}},{"source":["learning_rates = [0.01, 0.001, 0.0001]\n","models = {}\n","for i in learning_rates:\n","    print (\"learning rate is: \" + str(i))\n","    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False)\n","    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n","\n","for i in learning_rates:\n","    plt.plot(np.squeeze(models[str(i)][\"costs\"]), label= str(models[str(i)][\"learning_rate\"]))\n","\n","plt.ylabel('cost')\n","plt.xlabel('iterations (hundreds)')\n","\n","legend = plt.legend(loc='upper center', shadow=True)\n","frame = legend.get_frame()\n","frame.set_facecolor('0.90')\n","plt.show()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" #Interpretation\n"," Different learning rates give different costs and thus different predictions results.\n"," If the learning rate is too large (0.01), the cost may oscillate up and down.\n"," It may even diverge (though in this example,\n"," using 0.01 still eventually ends up at a good value for the cost).\n"," A lower cost doesn't mean a better model. You have to check if there is possibly overfitting.\n"," It happens when the training accuracy is a lot higher than the test accuracy.\n"," In deep learning, we usually recommend that you:\n"," Choose the learning rate that better minimizes the cost function.\n"," If your model overfits, use other techniques to reduce overfitting"],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}